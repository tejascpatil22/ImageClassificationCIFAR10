{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1agGD7uP493Q"
      },
      "outputs": [],
      "source": [
        "#https://medium.com/@kenneth.ca95/a-guide-to-transfer-learning-with-keras-using-resnet50-a81a4a28084b\n",
        "# Running the version as 1.x is optional, without that first line it will run the last version of tensorflow for Colab.\n",
        "\n",
        "#import keras\n",
        "import tensorflow as tf \n",
        "from tensorflow import keras\n",
        "import tensorflow.keras as K\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "import pandas as pd\n",
        "#from keras.utils import plot_model\n",
        "from keras.utils.vis_utils import plot_model\n",
        "#from keras.utils import np_utils\n",
        "from keras import utils as np_utils\n",
        "#from keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "pd.set_option('display.max_columns',None)#displaying long list of columns\n",
        "pd.set_option('display.max_rows', None)#displaying long list of rows\n",
        "pd.set_option('display.width', 1000)#width of window\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUEuZsobBTUF"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "# CIFAR-10 is a dataset with 60000 32x32 colour images grouped in 10 classes, that means 6000 images per class. \n",
        "# This is a dataset of 50,000 32x32 color training images and 10,000 test images, labeled over 10 categories.\n",
        "# The categories are airplane, automobile, beer, cat, deer, dog, frog, horse, ship, truck. \n",
        "# We can take advantage of the fact that these categories and a lot more are into the Imagenet collection.\n",
        "# Loading the CIFAR-10 datasets\n",
        "from keras.datasets import cifar10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29Bq-NQmCDgS"
      },
      "outputs": [],
      "source": [
        "# Preprocess data function\n",
        "# Now that the data is loaded, we are going to build a preprocess function for the data. \n",
        "# We have X as a numpy array of shape (m, 32, 32, 3) where m is the number of images, \n",
        "# 32 and 32 the dimensions, and 3 is because we use colored images. \n",
        "# We have a set of X for training and a set of X for validation. \n",
        "# Y is a numpy array of shape (m, ) that we want to be our labels. \n",
        "# Since we work with 10 different categories, we make use of one-hot encoding with a \n",
        "# function of Keras that makes our Y into a shape of (m, 10). That also applies for the validation.\n",
        "\n",
        "def preprocess_data(X,Y):\n",
        "  X_p = K.applications.xception.preprocess_input(X)\n",
        "  Y_p = K.utils.to_categorical(Y,10)\n",
        "  return X_p, Y_p\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4qZ9aHTEQUp",
        "outputId": "e13689d3-c470-4b12-bdfd-3f07b83c7f96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "y_train shape: (50000, 1)\n",
            "x_test shape: (10000, 32, 32, 3)\n",
            "y_test shape: (10000, 1)\n"
          ]
        }
      ],
      "source": [
        "# load and split data\n",
        "# The data, split between train and test sets:\n",
        "# (x_train, y_train), (x_test, y_test) = K.datasets.cifar100.load_data()\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "img_rows, img_cols = 32, 32\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "print('y_test shape:', y_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAs88uCaFMc3",
        "outputId": "af9914ef-49c6-4e86-d10b-432f6668895c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "y_train shape: (50000, 10)\n",
            "x_test shape: (10000, 32, 32, 3)\n",
            "y_test shape: (10000, 10)\n"
          ]
        }
      ],
      "source": [
        "# Preprocess data\n",
        "# Next, we are going to call our function with the parameters loaded from the Cifar 10 database.\n",
        "\n",
        "x_train, y_train = preprocess_data(x_train, y_train)\n",
        "x_test, y_test = preprocess_data(x_test, y_test)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "print('y_test shape:', y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDEpJYeWGK46"
      },
      "outputs": [],
      "source": [
        "# Using weights of a trained neural network\n",
        "# A pretrained model from the Keras Applications has the advantage of allowing you to use weights that\n",
        "# are already calibrated to make predictions. In this case, we use the weights from Imagenet \n",
        "# and the network is a Xception. The option include_top=False allows feature extraction by removing \n",
        "# the last dense layers. This let us control the output and input of the model.\n",
        "\n",
        "\n",
        "input_t = K.Input(shape=(128,128,3))\n",
        "# input_t = K.Input(shape=(224,224,3))\n",
        "# input_t = K.Input(shape=(32,32,3))\n",
        "xception_model = K.applications.Xception(include_top=False,\n",
        "                                    weights=\"imagenet\",\n",
        "                                    input_tensor=input_t)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlyKjCtPH3m3"
      },
      "outputs": [],
      "source": [
        "# In this case, we ‘freeze’ all layers except for the last few block of the Exception.\n",
        "\n",
        "for layer in xception_model.layers[:95]:\n",
        "  layer.trainable=False\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdixTKJ7I10V",
        "outputId": "3d207226-eec0-48b9-96ea-e2fc3bb46320"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 input_4 - False\n",
            "1 block1_conv1 - False\n",
            "2 block1_conv1_bn - False\n",
            "3 block1_conv1_act - False\n",
            "4 block1_conv2 - False\n",
            "5 block1_conv2_bn - False\n",
            "6 block1_conv2_act - False\n",
            "7 block2_sepconv1 - False\n",
            "8 block2_sepconv1_bn - False\n",
            "9 block2_sepconv2_act - False\n",
            "10 block2_sepconv2 - False\n",
            "11 block2_sepconv2_bn - False\n",
            "12 conv2d_12 - False\n",
            "13 block2_pool - False\n",
            "14 batch_normalization_21 - False\n",
            "15 add_36 - False\n",
            "16 block3_sepconv1_act - False\n",
            "17 block3_sepconv1 - False\n",
            "18 block3_sepconv1_bn - False\n",
            "19 block3_sepconv2_act - False\n",
            "20 block3_sepconv2 - False\n",
            "21 block3_sepconv2_bn - False\n",
            "22 conv2d_13 - False\n",
            "23 block3_pool - False\n",
            "24 batch_normalization_22 - False\n",
            "25 add_37 - False\n",
            "26 block4_sepconv1_act - False\n",
            "27 block4_sepconv1 - False\n",
            "28 block4_sepconv1_bn - False\n",
            "29 block4_sepconv2_act - False\n",
            "30 block4_sepconv2 - False\n",
            "31 block4_sepconv2_bn - False\n",
            "32 conv2d_14 - False\n",
            "33 block4_pool - False\n",
            "34 batch_normalization_23 - False\n",
            "35 add_38 - False\n",
            "36 block5_sepconv1_act - False\n",
            "37 block5_sepconv1 - False\n",
            "38 block5_sepconv1_bn - False\n",
            "39 block5_sepconv2_act - False\n",
            "40 block5_sepconv2 - False\n",
            "41 block5_sepconv2_bn - False\n",
            "42 block5_sepconv3_act - False\n",
            "43 block5_sepconv3 - False\n",
            "44 block5_sepconv3_bn - False\n",
            "45 add_39 - False\n",
            "46 block6_sepconv1_act - False\n",
            "47 block6_sepconv1 - False\n",
            "48 block6_sepconv1_bn - False\n",
            "49 block6_sepconv2_act - False\n",
            "50 block6_sepconv2 - False\n",
            "51 block6_sepconv2_bn - False\n",
            "52 block6_sepconv3_act - False\n",
            "53 block6_sepconv3 - False\n",
            "54 block6_sepconv3_bn - False\n",
            "55 add_40 - False\n",
            "56 block7_sepconv1_act - False\n",
            "57 block7_sepconv1 - False\n",
            "58 block7_sepconv1_bn - False\n",
            "59 block7_sepconv2_act - False\n",
            "60 block7_sepconv2 - False\n",
            "61 block7_sepconv2_bn - False\n",
            "62 block7_sepconv3_act - False\n",
            "63 block7_sepconv3 - False\n",
            "64 block7_sepconv3_bn - False\n",
            "65 add_41 - False\n",
            "66 block8_sepconv1_act - False\n",
            "67 block8_sepconv1 - False\n",
            "68 block8_sepconv1_bn - False\n",
            "69 block8_sepconv2_act - False\n",
            "70 block8_sepconv2 - False\n",
            "71 block8_sepconv2_bn - False\n",
            "72 block8_sepconv3_act - False\n",
            "73 block8_sepconv3 - False\n",
            "74 block8_sepconv3_bn - False\n",
            "75 add_42 - False\n",
            "76 block9_sepconv1_act - False\n",
            "77 block9_sepconv1 - False\n",
            "78 block9_sepconv1_bn - False\n",
            "79 block9_sepconv2_act - False\n",
            "80 block9_sepconv2 - False\n",
            "81 block9_sepconv2_bn - False\n",
            "82 block9_sepconv3_act - False\n",
            "83 block9_sepconv3 - False\n",
            "84 block9_sepconv3_bn - False\n",
            "85 add_43 - False\n",
            "86 block10_sepconv1_act - False\n",
            "87 block10_sepconv1 - False\n",
            "88 block10_sepconv1_bn - False\n",
            "89 block10_sepconv2_act - False\n",
            "90 block10_sepconv2 - False\n",
            "91 block10_sepconv2_bn - False\n",
            "92 block10_sepconv3_act - False\n",
            "93 block10_sepconv3 - False\n",
            "94 block10_sepconv3_bn - False\n",
            "95 add_44 - True\n",
            "96 block11_sepconv1_act - True\n",
            "97 block11_sepconv1 - True\n",
            "98 block11_sepconv1_bn - True\n",
            "99 block11_sepconv2_act - True\n",
            "100 block11_sepconv2 - True\n",
            "101 block11_sepconv2_bn - True\n",
            "102 block11_sepconv3_act - True\n",
            "103 block11_sepconv3 - True\n",
            "104 block11_sepconv3_bn - True\n",
            "105 add_45 - True\n",
            "106 block12_sepconv1_act - True\n",
            "107 block12_sepconv1 - True\n",
            "108 block12_sepconv1_bn - True\n",
            "109 block12_sepconv2_act - True\n",
            "110 block12_sepconv2 - True\n",
            "111 block12_sepconv2_bn - True\n",
            "112 block12_sepconv3_act - True\n",
            "113 block12_sepconv3 - True\n",
            "114 block12_sepconv3_bn - True\n",
            "115 add_46 - True\n",
            "116 block13_sepconv1_act - True\n",
            "117 block13_sepconv1 - True\n",
            "118 block13_sepconv1_bn - True\n",
            "119 block13_sepconv2_act - True\n",
            "120 block13_sepconv2 - True\n",
            "121 block13_sepconv2_bn - True\n",
            "122 conv2d_15 - True\n",
            "123 block13_pool - True\n",
            "124 batch_normalization_24 - True\n",
            "125 add_47 - True\n",
            "126 block14_sepconv1 - True\n",
            "127 block14_sepconv1_bn - True\n",
            "128 block14_sepconv1_act - True\n",
            "129 block14_sepconv2 - True\n",
            "130 block14_sepconv2_bn - True\n",
            "131 block14_sepconv2_act - True\n"
          ]
        }
      ],
      "source": [
        "# We can check that we did it correctly with:\n",
        "# False means that the layer is ‘freezed’ or is not trainable and \n",
        "# True that when we run our model, the weights are going to be adjusted.\n",
        "\n",
        "for i, layer in enumerate(xception_model.layers):\n",
        "  print(i,layer.name,\"-\",layer.trainable)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PHC3U6xJceX"
      },
      "outputs": [],
      "source": [
        "    # Add Flatten and Dense layers on top of Xception\n",
        "    # Now, we need to connect our pretrained model with the new layers \n",
        "    # of our model. We can use global pooling or a flatten layer to connect \n",
        "    # the dimensions of the previous layers with the new layers. \n",
        "    \n",
        "    to_res = (128, 128)\n",
        "    # to_res = (224, 224)\n",
        "    # to_res = (32, 32)\n",
        "    model = K.models.Sequential()\n",
        "    model.add(K.layers.Lambda(lambda image: tf.image.resize(image, to_res))) \n",
        "    model.add(xception_model)\n",
        "    model.add(K.layers.Flatten())\n",
        "    model.add(K.layers.BatchNormalization())\n",
        "    model.add(K.layers.Dense(256, activation='relu'))\n",
        "    model.add(K.layers.Dropout(0.1))\n",
        "    model.add(K.layers.BatchNormalization())\n",
        "    model.add(K.layers.Dense(128, activation='relu'))\n",
        "    model.add(K.layers.Dropout(0.1))\n",
        "    model.add(K.layers.BatchNormalization())\n",
        "   # model.add(K.layers.Dense(64, activation='relu'))\n",
        "   # model.add(K.layers.Dropout(0.1))\n",
        "   # model.add(K.layers.BatchNormalization())\n",
        "    model.add(K.layers.Dense(10, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NClKTTPoJwjA",
        "outputId": "bbb2c8a7-0756-44b5-fe39-5c9e3d6683a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1563/1563 [==============================] - 136s 84ms/step - loss: 0.5330 - accuracy: 0.8301 - val_loss: 0.3460 - val_accuracy: 0.8890\n",
            "Epoch 2/5\n",
            "1563/1563 [==============================] - 129s 83ms/step - loss: 0.2874 - accuracy: 0.9086 - val_loss: 0.3376 - val_accuracy: 0.8945\n",
            "Epoch 3/5\n",
            "1563/1563 [==============================] - 129s 83ms/step - loss: 0.1964 - accuracy: 0.9365 - val_loss: 0.2914 - val_accuracy: 0.9087\n",
            "Epoch 4/5\n",
            "1563/1563 [==============================] - 136s 87ms/step - loss: 0.1415 - accuracy: 0.9541 - val_loss: 0.3027 - val_accuracy: 0.9141\n",
            "Epoch 5/5\n",
            "1563/1563 [==============================] - 130s 83ms/step - loss: 0.1063 - accuracy: 0.9661 - val_loss: 0.3070 - val_accuracy: 0.9129\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lambda_3 (Lambda)           (None, 128, 128, 3)       0         \n",
            "                                                                 \n",
            " xception (Functional)       (None, 4, 4, 2048)        20861480  \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 32768)             0         \n",
            "                                                                 \n",
            " batch_normalization_25 (Bat  (None, 32768)            131072    \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 256)               8388864   \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " batch_normalization_26 (Bat  (None, 256)              1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " batch_normalization_27 (Bat  (None, 128)              512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 29,417,138\n",
            "Trainable params: 18,505,690\n",
            "Non-trainable params: 10,911,448\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Compile model and train\n",
        "# Results\n",
        "# In the 8th epoch, the values are very similar and it is interesting to note that \n",
        "# in the first validation accuracy is higher than training. \n",
        "# This is because of dropout use, which in Keras, it has a different behavior \n",
        "# for training and testing. In testing time, all the features are ready and \n",
        "# the dropout is turned off, resulting in a better accuracy. \n",
        "# This readjust on the last epochs since the model continues changing on the training.\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "history = model.fit(x_train, y_train, batch_size=32, epochs=5, verbose=1,\n",
        "                        validation_data=(x_test, y_test)\n",
        "                       )\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Changes in Final Model:\n",
        "- Dropout reduced to **0.1** <br>\n",
        "- Img Size increased to **224 x 224 x 3** <br>\n",
        "- Optimizer changed to **\"adam\"** <br>\n",
        "- No. of layers frozen increased from 75 to **95** <br>"
      ],
      "metadata": {
        "id": "qc-QxtW-LhfX"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}